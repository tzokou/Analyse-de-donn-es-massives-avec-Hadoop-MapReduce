#!/usr/bin/env bash
# ============================================================
#  Script pipeline UA2 - Hadoop MapReduce automatis√©
#  Objet : Automatiser le nettoyage, le d√©p√¥t HDFS, le traitement MapReduce et la r√©cup√©ration des r√©sultats pour un pipeline analytique de ventes multicanal.
#  Pr√©requis :
#    - Docker Hadoop op√©rationnel (cluster type mzinee/hadoop-cluster)
#    - Fichier .env correctement param√©tr√© (variables chemins et jobs)
#    - Tous les scripts Python MapReduce sont dans le r√©pertoire src/
# ============================================================

set -Eeuo pipefail

echo "=== [UA2] Pipeline Hadoop MapReduce ==="

# 1Ô∏è‚É£ Charger le fichier .env s'il existe (sinon, arr√™t avec message clair)
if [ -f ".env" ]; then
  set -a                   
  source .env
  set +a
else
  echo "[ERREUR] Fichier .env introuvable. Arr√™t du script."
  exit 1
fi

# 2Ô∏è‚É£ V√©rification des variables essentielles
REQUIRED_VARS=(RAW_DIR RAW_SALES RAW_SALES_INCREMENT RAW_PRODUCTS HDFS_ROOT HDFS_INPUT HDFS_CLEAN HDFS_OUTPUT)
for var in "${REQUIRED_VARS[@]}"; do
  if [ -z "${!var:-}" ]; then
    echo "[ERREUR] Variable d'environnement manquante : $var"
    exit 1
  fi
done

PY_BIN="${PY_BIN:-python3}"

# === √âtape 1 : Nettoyage local
echo "=== [1/4] Nettoyage et pr√©paration locale ==="
$PY_BIN src/clean_and_join.py || { echo "[ERREUR] √âchec du nettoyage local."; exit 1; }

# === √âtape 2 : Publication sur HDFS
echo "=== [2/4] Publication du fichier clean.csv sur HDFS ==="
hdfs dfs -rm -r -f "$HDFS_ROOT" || true
hdfs dfs -mkdir -p "$HDFS_INPUT" "$HDFS_CLEAN" "$HDFS_OUTPUT"

if [ ! -f "$CLEAN_DIR/clean.csv" ]; then
  echo "[ERREUR] Le fichier $CLEAN_DIR/clean.csv est introuvable. Arr√™t du script."
  exit 1
fi

hdfs dfs -put -f "$CLEAN_DIR/clean.csv" "$HDFS_CLEAN/clean.csv"
echo "‚úÖ Fichier transf√©r√© sur HDFS : $HDFS_CLEAN/clean.csv"

# === √âtape 3 : Lancement des jobs MapReduce dans src/ ===
echo "=== [3/4] Ex√©cution des jobs MapReduce ==="
echo "‚Üí Job 1 : KPI Country-Month"
$PY_BIN src/job_kpi_sales_by_country_month.py -r hadoop "$HDFS_CLEAN/clean.csv" \
  --output-dir "$HDFS_OUTPUT/kpi_country_month" --no-output

echo "‚Üí Job 2 : Top $TOPN produits"
$PY_BIN src/job_top10_products.py -r hadoop "$HDFS_CLEAN/clean.csv" \
  --output-dir "$HDFS_OUTPUT/top_products" --no-output

echo "‚Üí Job 3 : Taux de retour"
$PY_BIN src/job_return_rate.py -r hadoop "$HDFS_CLEAN/clean.csv" \
  --output-dir "$HDFS_OUTPUT/return_rate" --no-output

# === √âtape 4 : Rapatriement local des fichiers r√©sultats ===
echo "=== [4/4] R√©cup√©ration des r√©sultats ==="
mkdir -p "$WORK_DIR/results"

hdfs dfs -get -f "$HDFS_OUTPUT/top_products/part-*" "$WORK_DIR/results/top_products.csv" || echo "[WARN] R√©sultat top_products manquant."
hdfs dfs -get -f "$HDFS_OUTPUT/kpi_country_month/part-*" "$WORK_DIR/results/kpi_country_month.csv" || echo "[WARN] R√©sultat KPI manquant."
hdfs dfs -get -f "$HDFS_OUTPUT/return_rate/part-*" "$WORK_DIR/results/return_rate.jsonl" || echo "[WARN] R√©sultat return_rate manquant."

echo ""
echo "‚úÖ Pipeline Hadoop MapReduce termin√© avec succ√®s."
echo "üìÇ R√©sultats locaux : $WORK_DIR/results/"
echo "üìÅ Donn√©es HDFS     : $HDFS_CLEAN/clean.csv et $HDFS_OUTPUT/"
echo "==============================================================="

